{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7viWdoeRyDL"
   },
   "source": [
    "## Step 1: Install Some Software\n",
    "\n",
    "\n",
    "1. Install Google's Chrome Webbrowser on your computer (if you don't already have it installed): https://www.google.com/chrome/   \n",
    "\n",
    "\n",
    "2. Create a Folder (Directory) in your computer where you plan to do your scraping (i.e., run the scraper from and save scraping results to). \n",
    "\n",
    "3. Download Chromedriver https://chromedriver.chromium.org/downloads to the directory that you plan to scrape form/to (see 2. above)/\n",
    "    - <font color='red'>Make sure to select the version of chromedriver that matches your Chrome Browser installation</font>\n",
    "\n",
    "    - Check your Chrome version from the Chrome Browser's menu \n",
    "    * upper-right corner-->Help-->about Google Chrome, ***or*** \n",
    "    * upper-left corner-->Chrome-->About Google Chrome  \n",
    "      \n",
    "    - Download the version that is appropriate for your operating system. I have a MacBook with an Intel processor and with Google Chrome version 98 installed, so I will download ***Chrome version 98 --> chromedriver_mac64.zip***  \n",
    "      \n",
    "      \n",
    "4. Go to your folder (where you downloaded the Chromedriver to), unpack it, and double-click the file to run it once. A command prompt / terminal will pop-up - Wait until it prints out a successful message before closing it. (If you are a Mac user, and it's the first time you open it, you will see an error message now allowing to to open it. Go to System Preference on Mac --> Security & Privacy --> General--> Open Anyway)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmLjdYFURyDM"
   },
   "source": [
    "## Step 2: Create Python Environment and Install Python Libraries to it\n",
    "\n",
    "We need three libraries to scrape hotel reviews with this notebook:\n",
    "- **Selenium**: Selenium refers to a suite of tools that are widely used in the testing community when it comes to cross-browser testing. Selenium cannot automate desktop applications; it can only be used in browsers. It is considered to be one of the most preferred tool suites for automation testing of web applications as it provides support for popular web browsers which makes it very powerful.   \n",
    "  \n",
    "\n",
    "- **beautifulsoup4**: Beautiful Soup is a library that makes it easy to scrape information from web pages. It sits atop an HTML or XML parser, providing Pythonic idioms for iterating, searching, and modifying the parse tree.  \n",
    "\n",
    "  \n",
    "- **Pandas**: pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
    "built on top of the Python programming language.\n",
    "\n",
    "Before we get started, we need to create a python environment on our computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHx27z2_RyDN"
   },
   "source": [
    "### Create Python Environment\n",
    "\n",
    "\n",
    "1. You should have already created a folder on your computer where you want the scraped reviews to be stored. Chromedriver should also be located in this folder. For example: \n",
    "\n",
    "2. Open your Terminal (Mac) or Anaconda/Command Prompt (Windows) and navigate to that folder:\n",
    "\n",
    "3. Make sure your Python and Anaconda are up to date:\n",
    "\n",
    "  ```\n",
    "  conda update conda --yes\n",
    "  conda update anaconda --yes\n",
    "  conda update python --yes\n",
    "  conda update --all --yes\n",
    "  ```\n",
    "\n",
    "4. Create a new Python Environment if you have not already done so by cloning your base environment (this can take a moment):\n",
    "\n",
    "  ```\n",
    "  conda create --name Reviews --clone base\n",
    "  conda activate Reviews\n",
    "  python -m ipykernel install --user --name Reviews --display-name “Reviews”\n",
    "  ```\n",
    "\n",
    "5. Make sure you have activated your new environment:\n",
    "\n",
    "  ```\n",
    "  conda activate Reviews\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrTGUO8WRyDO"
   },
   "source": [
    "### Install Libraries to New Environment\n",
    "\n",
    "In your terminal (anaconde / command prompt), type the following to install the required libraries:\n",
    "\n",
    "  ```\n",
    "  conda activate Reviews\n",
    "  ```\n",
    "  \n",
    "  ```\n",
    "  pip install pandas\n",
    "  ```\n",
    "  \n",
    "  ```\n",
    "  pip install selenium\n",
    "  ```\n",
    "  \n",
    "  ```\n",
    "  pip install beautifulsoup4\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwAfWdsgRyDO"
   },
   "source": [
    "### Reload Notebook and Change Kernel\n",
    "To use our new environment, we need to reload the notebook and change the kernel\n",
    "- Click on File > Close and Halt\n",
    "- Go back to the Jupyter tab in your browser and load the notebook again\n",
    "- Click on Kernel > Change Kernel > Reviews\n",
    "\n",
    "You should see \"Reviews\" in the top right-hand corner of your notebook (right under the \"Logout\" button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lN38JRxzRyDP"
   },
   "source": [
    "##  Step 3: Import some Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xV2MsgMRyDP"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bogjb1StRyDQ"
   },
   "source": [
    "## The ReviewScraper Code\n",
    "\n",
    "- We wrote this scraper specifically for this course so that you can use it in your team assignment.\n",
    "- You don't need to understand the code in detail. That is beyond this course\n",
    "- You will need to use the code if you want to collect reviews from the internet.\n",
    "\n",
    "Please run through the next three cells, which include the review scraper class and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqW_0k97RyDQ"
   },
   "outputs": [],
   "source": [
    "class ReviewScraper:\n",
    "    def __init__(self, path_to_driver, site):\n",
    "        \"\"\"\n",
    "        :param path_to_driver: the path to your ChromeDriver. \n",
    "        \n",
    "        \"\"\"\n",
    "        self.path_to_driver = path_to_driver\n",
    "        self.site = site\n",
    "        \n",
    "    def scrapeGoogle(self,\n",
    "                url, \n",
    "                driver, \n",
    "                scroll=True,\n",
    "                slptime=2.25,\n",
    "                creview = \"Svr5cf bKhjM\",\n",
    "                treview = 'div',\n",
    "                ctime_site = \"iUtr1\",\n",
    "                ttime_site = 'span',\n",
    "                cname = \"DHIhE\" ,\n",
    "                cnameNotInGoogle = \"faBUBf\",\n",
    "                tname = 'a',\n",
    "                tnameNotInGoogle = 'span',\n",
    "                crating = \"MfbzKb\",\n",
    "                trating = 'div',\n",
    "                ctripType = \"VURE3b\",\n",
    "                ttripType = 'span',\n",
    "                crsl = \"dA5Vzb\",\n",
    "                trsl = 'div',\n",
    "                ctext = 'K7oBsc',\n",
    "                ttext = 'div',\n",
    "                chtname = 'CQYfx hAP9Pd gEBR9d',\n",
    "                thtname = 'a',\n",
    "                calthtname = 'nCqM5e',\n",
    "                talthtname = 'div',\n",
    "                creply = 'n7uVJf',\n",
    "                treply = 'div',\n",
    "                creplydate = 'a0th8b',\n",
    "                treplydate = 'span'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Scrapes results from a hotel review page from Google Travel\n",
    "        :param url: the url to the review site\n",
    "        :param driver: the web driver object\n",
    "        :param scroll: whether or not to scroll down until all the reviews are available for scraping. \n",
    "        \n",
    "        # HTML tag information\n",
    "        :param creview: class name of the tag containing the whole review section  \n",
    "        :param treview: type of the tag containing the whole review section  \n",
    "        \n",
    "        :param cscroll: class name of tag containing each scroll of ten reviews\n",
    "        :param tscroll: type of tag containing each scroll of ten reviews\n",
    "        \n",
    "        :param ctime_site: class name of tag containing time and site\n",
    "        :param ttime_site: type of tag containing time and site\n",
    "        \n",
    "        :param cname: class name of the tag containing reviewer's name if the review is from Google\n",
    "        :param cnameNotInGoogle: class name of the tag containing reviewer's name if the review is not from Google\n",
    "        :param tname: type of tag containing reviewer's name if the review is from Google\n",
    "        :param tnameNotInGoogle: type of tag containing reviewer's name if the review is not from Google\n",
    "        \n",
    "        :param crating: class name of tag containing overall rating \n",
    "        :param trating: type of tag containing overall rating \n",
    "\n",
    "        :param ctripType: class name of tag containing trip type \n",
    "        :param ttripType: type of tag containing trip type \n",
    "\n",
    "        # the tags containing ratings for rooms, services, and location (r,s,l) all have the same class and type, so we scrape them together \n",
    "        :param crsl: class name of tag containing r,s,l\n",
    "        :param trsl: type of tag containing r,s,l\n",
    "\n",
    "        :param ctext: class name of tag containing review text. There may be multiple components within each review\n",
    "        :param ttext: type of tag containing review text. There may be multiple components within each review\n",
    "        \n",
    "        :param chtname: class name of tag containing the hotel name.\n",
    "        :param thtname: type of tag containing the hotel name.\n",
    "        \n",
    "        # The alternative hotel name is at the bottom of the page\n",
    "        :param calthtname: class of tag containing the alternatve location of the hotel name (in case it does not load using the previous tag)\n",
    "        :param talthtname: type of tag containing the alternatve location of the hotel name (in case it does not load using the previous tag)\n",
    "        \n",
    "        :param creply: class of tag containing all the hotel's reply\n",
    "        :param treply: type of tag containing all the hotel's reply\n",
    "        \n",
    "        :param creplydate: class of tag containing the reply time \n",
    "        :param treplydate: type of tag containing the reply time \n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Collecting data from specified URL ...\")\n",
    "        driver.get(url)  \n",
    "        time.sleep(5)\n",
    "        driver.set_window_size(1050,660)\n",
    "\n",
    "        if scroll:\n",
    "            while True:  \n",
    "                print(\"Scrolling down to get all information... \\n\\n**** Do not manipulate the chrome browser window in any way ****\\n\\n\")\n",
    "                print(\"Depending on the total number of reviews, this may take a long time.\")\n",
    "                print(\"Do not allow your computer or screen to go to sleep during the scraping process.\")\n",
    "                scroll_down(driver, sleepTime=slptime)\n",
    "                time.sleep(slptime)\n",
    "                lctime = time.time()\n",
    "                slptime += 1\n",
    "                scroll_down(driver, sleepTime=slptime)\n",
    "                if time.time() - lctime > slptime+2:\n",
    "                    print(\"WARNING: Your sleeptime may be too slow, adjusting and restarting scroll...\")\n",
    "                    continue\n",
    "                break\n",
    "                \n",
    "\n",
    "        print(\"Starting data extraction (parsing HTML code) ...\")\n",
    "        content = driver.page_source\n",
    "\n",
    "        months, names, ratings, sites, tripTypes, rooms, locations, services, ids, texts, replys, replydates = (list() for l in range(12))\n",
    "        \n",
    "        soup = BeautifulSoup(content)\n",
    "        \n",
    "        # get hotel name         \n",
    "        htnameTag = soup.find(thtname, class_=chtname)\n",
    "        if htnameTag is None:\n",
    "            htnameTag = soup.find(talthtname, class_=calthtname)\n",
    "            assert htnameTag is not None, \"Hotel name cannot be found in any of the two possible spots\"\n",
    "        htname = htnameTag.get_text()\n",
    "        print('Collecting data for: ' + htname)\n",
    "\n",
    "        # parse each review\n",
    "        for review in soup.findAll(treview, class_=creview):\n",
    "            soup2 = BeautifulSoup(str(review))\n",
    "\n",
    "            # get name of each reviewer\n",
    "            name = soup2.find(tname, class_=cname)\n",
    "            if name is None:\n",
    "                names.append(soup2.find(tnameNotInGoogle, class_=cnameNotInGoogle).get_text())\n",
    "            else:\n",
    "                names.append(name.get_text())\n",
    "\n",
    "            # get review time and site \n",
    "            time_site = soup2.find(ttime_site, class_=ctime_site).get_text()\n",
    "            month = ' '.join(time_site.split(' ')[:-3])\n",
    "            site = time_site.split(' ')[-1]\n",
    "            months.append(month)\n",
    "            sites.append(site)\n",
    "\n",
    "            # get overall rating \n",
    "            ratings.append(soup2.find(trating, class_=crating).get_text())\n",
    "\n",
    "            # get trip type\n",
    "            tripType = soup2.find(ttripType, class_=ctripType)\n",
    "            tripTypes.append('N/A' if tripType is None else tripType.get_text())\n",
    "\n",
    "            # get ratings for rooms, services, and locations (any number of any of these may appear)\n",
    "            \n",
    "            rsl = soup2.findAll(trsl, class_=crsl)\n",
    "            if len(rsl) == 0:\n",
    "                rooms.append('N/A')\n",
    "                services.append('N/A')\n",
    "                locations.append('N/A')\n",
    "            else:\n",
    "                hasRoom = False\n",
    "                hasServices = False\n",
    "                hasLocations = False\n",
    "                for rt in rsl:\n",
    "                    txt = rt.get_text()\n",
    "                    if 'Room' in txt:\n",
    "                        rooms.append(txt[-3:])\n",
    "                        hasRoom = True\n",
    "                    elif 'Serv' in txt:\n",
    "                        services.append(txt[-3:])\n",
    "                        hasServices = True\n",
    "                    elif 'Loc' in txt:\n",
    "                        locations.append(txt[-3:])\n",
    "                        hasLocations = True\n",
    "                    else:\n",
    "                        assert False, \"rsl tag found, but none of rsl appeared\"\n",
    "                if not hasRoom:\n",
    "                    rooms.append(\"N/A\")\n",
    "                if not hasServices:\n",
    "                    services.append(\"N/A\")\n",
    "                if not hasLocations:\n",
    "                    locations.append(\"N/A\")\n",
    "                assert len(rooms) == len(services) == len(locations), \"wrong array length after appending rsl ratings\"        \n",
    "\n",
    "            # merge all review text segments (the may be multiple segments)\n",
    "            textSegs = soup2.findAll(ttext, class_=ctext)\n",
    "#             print(textSegs[-1].get_text())\n",
    "#             allText = list()\n",
    "#             for segment in textSegs:\n",
    "#                 if len(textSegs) > 1:\n",
    "#                     print(segment.get_text())\n",
    "#                 allText.append(segment.get_text())\n",
    "#             text = ' '.join(allText)\n",
    "#             print(len(textSegs))\n",
    "\n",
    "#             texts.append(textSegs[-1].get_text() if len(textSegs)!=0 else \"N/A\")\n",
    "            if len(textSegs)!=0:\n",
    "                texts.append(BeautifulSoup(str(textSegs[-1]).replace('<br/>', '  ')).get_text().replace('Â¿', \"'\"))\n",
    "            else:\n",
    "                texts.append('N/A')\n",
    "    \n",
    "            # attach hotel reply text\n",
    "            reps = soup2.findAll(treply, class_=creply)\n",
    "            assert len(reps)<2, \"text segment error, please contact Ziyao\"\n",
    "#             replys.append(reps[0].get_text() if len(reps)==1 else 'N/A')\n",
    "            if len(reps)==1:\n",
    "                repind = str(reps[0]).rindex('</span>')+7\n",
    "                replytext = BeautifulSoup(str(reps[0])[repind:].replace('</p>', ' ').replace('</span>', ' ')).get_text()\n",
    "                replys.append(replytext)\n",
    "                \n",
    "                # append reply date\n",
    "                replydates.append(BeautifulSoup(str(reps[0])).find(treplydate, class_=creplydate).get_text())\n",
    "            else:\n",
    "                replys.append(\"N/A\")\n",
    "                replydates.append(\"N/A\")\n",
    "\n",
    "        # generate id for all ratings for each hotel\n",
    "#         ids = np.arange(len(services))   \n",
    "        htnames = [htname] * len(services)\n",
    "\n",
    "        assert len(replydates) == len(replys) == len(names) == len(months) == len(ratings) == len(sites) == len(tripTypes) == len(rooms) == len(locations) == len(services) == len(texts) == len(htnames), 'different number of inputs for some review features'\n",
    "\n",
    "        # convert each review feature list into numpy arrays\n",
    "        months = np.array(months)\n",
    "        names = np.array(names)\n",
    "        ratings = np.array(ratings).astype('str')\n",
    "        sites = np.array(sites)\n",
    "        tripTypes = np.array(tripTypes)\n",
    "        rooms = np.array(rooms)\n",
    "        locations = np.array(locations)\n",
    "        services = np.array(services)\n",
    "#         ids = np.array(ids)\n",
    "        texts = np.array(texts)\n",
    "        htnames = np.array(htnames)\n",
    "        replys = np.array(replys)\n",
    "        replydates = np.array(replydates)\n",
    "\n",
    "        # vertically stack each feature together\n",
    "        allInfo = np.vstack([htnames, names,months,ratings,sites,tripTypes,rooms,locations,services,texts,replys, replydates])\n",
    "        #print('shape of final array:')\n",
    "        #print(allInfo.shape)\n",
    "\n",
    "        return allInfo    \n",
    "\n",
    "    \n",
    "   \n",
    "    def scrapeAdvisor(self, url, driver, scroll=True):\n",
    "        \"\"\"\n",
    "        Scrapes results from a hotel review page from trip advisor\n",
    "        :param url: the url to the review site\n",
    "        :param driver: the web driver object\n",
    "        :param scroll: whether or not to press \"next page\" until all the reviews are gathered \n",
    "        \"\"\"\n",
    "        \n",
    "        # class and type of tag containing both the reviewer username and the review time\n",
    "        cname_time = 'bcaHz'\n",
    "        tname_time = 'div'\n",
    "        \n",
    "        # class and type tag containing of reviewer username (unverified/verified)\n",
    "        cname = \"ui_header_link bPvDb\"\n",
    "        tname = 'a'\n",
    "        cnameverified = 'ui_header_link bPvDb verified'\n",
    "        tnameverified = 'a'\n",
    "        \n",
    "        # class and type of the tag containing each review block\n",
    "        creview = \"cWwQK MC R2 Gi z Z BB dXjiy\"\n",
    "        treview = 'div'\n",
    "\n",
    "        # class and type of the tag containing the rating, we use the tag containing the span \n",
    "        crating = \"emWez F1\"\n",
    "        trating = 'div'\n",
    "\n",
    "        # class and type of the tag containing the review title, we use the tag containing the span\n",
    "        ctitle = \"fCitC\"\n",
    "        ttitle = 'a'\n",
    "\n",
    "        # class and type of tag containing all other optional ratings \n",
    "        coptional = \"cnFkU Me f\"\n",
    "        toptional = 'div'\n",
    "\n",
    "        # class and type of tag of the review text\n",
    "        ctext = \"XllAv H4 _a\"\n",
    "        ttext = 'q'\n",
    "\n",
    "        # class of tag of expand button\n",
    "        cexpand = 'eljVo _S Z'\n",
    "        \n",
    "        # class and type of tag of hotel name\n",
    "        chtname = 'fkWsC b d Pn'\n",
    "        thtname = 'h1'\n",
    "        \n",
    "        # class and type of the number of contributions as well as the number of upvotes (same)\n",
    "        ccontributions = 'ckXjS'\n",
    "        tcontributions = 'span'\n",
    "        \n",
    "        # class and type of the time of stay\n",
    "        ctimestay = \"euPKI _R Me S4 H3\"\n",
    "        ttimestay = \"span\"\n",
    "        \n",
    "        # class and type of reviewer location \n",
    "        creviewerloc = \"default ShLyt small\"\n",
    "        treviewerloc = \"span\"\n",
    "        \n",
    "        # class and type of the hotel's reply\n",
    "        creply = \"eBsXT _a\"\n",
    "        treply = 'span'\n",
    "        \n",
    "        #class and type of travel type \n",
    "        ctptype = 'eHSjO _R Me'\n",
    "        ttptype = 'span'\n",
    "        \n",
    "        #sleep time for DOM reload\n",
    "        sleepTime = 2.5\n",
    "\n",
    "        htnames, times, names, ratings, titles, values, rooms, locations, cleanliness, services, slpqualitys, texts, numcontributions, timestays, reviewerlocs, numupvotes, replys, tptypes = (list() for i in range(18))\n",
    "        \n",
    "        print(\"getting url...\")\n",
    "        driver.get(url) \n",
    "        time.sleep(5)\n",
    "        \n",
    "        # get hotel name   \n",
    "        content = driver.page_source\n",
    "        soup = BeautifulSoup(content)\n",
    "        htnameTag = soup.find(thtname, class_=chtname)\n",
    "        assert htnameTag is not None, \"Hotel name not found\"\n",
    "        htname = htnameTag.get_text()\n",
    "        print('Collecting data for: ' + htname)\n",
    "        \n",
    "        \n",
    "        while True:\n",
    "            content = driver.page_source\n",
    "            soup = BeautifulSoup(content)\n",
    "            all_reviews = soup.findAll(treview, class_=creview)\n",
    "\n",
    "            for review in all_reviews:\n",
    "                soup2 = BeautifulSoup(str(review))\n",
    "                name_time = soup2.findAll(tname_time, class_=cname_time)\n",
    "\n",
    "                # add review time \n",
    "                times.append(str(name_time).split(\"wrote a review \")[1].split('</')[0])\n",
    "\n",
    "                #add reviewer name\n",
    "                reviewer = soup2.find(tname, class_=cname)\n",
    "                if reviewer is None:\n",
    "                    reviewer = soup2.find(tnameverified, class_=cnameverified)\n",
    "                names.append(reviewer.get_text())\n",
    "\n",
    "                #add overall rating\n",
    "                ratings.append(str(soup2.find(trating, class_=crating)).split(\"ui_bubble_rating bubble_\")[1][0])\n",
    "\n",
    "                # add review title\n",
    "                titles.append(soup2.find(ttitle, class_=ctitle).get_text())\n",
    "\n",
    "                # add all optional ratings\n",
    "                optionals = soup2.find(toptional, class_=coptional)\n",
    "                if optionals is None:\n",
    "#                     print(\"None\")\n",
    "                    values.append('N/A') \n",
    "                    rooms.append('N/A')\n",
    "                    locations.append('N/A')\n",
    "                    cleanliness.append('N/A')\n",
    "                    services.append('N/A')\n",
    "                    slpqualitys.append('N/A')\n",
    "                else:\n",
    "                    op = [j for s in str(optionals).split('</span></div>') for j in s.split('</span></span><span>')]\n",
    "                    values.append(op[op.index('Value')-1][-4] if 'Value' in op else \"N/A\")\n",
    "                    rooms.append(op[op.index('Rooms')-1][-4] if 'Rooms' in op else \"N/A\")\n",
    "                    locations.append(op[op.index('Location')-1][-4] if 'Location' in op else \"N/A\")\n",
    "                    cleanliness.append(op[op.index('Cleanliness')-1][-4] if 'Cleanliness' in op else \"N/A\")\n",
    "                    services.append(op[op.index('Service')-1][-4] if 'Service' in op else \"N/A\")\n",
    "                    slpqualitys.append(op[op.index('Sleep Quality')-1][-4] if 'Sleep Quality' in op else \"N/A\")\n",
    "\n",
    "                #add review text \n",
    "                texts.append(BeautifulSoup(str(soup2.find(ttext, class_=ctext)).replace('<br/>', ' ')).get_text())\n",
    "#                 print(BeautifulSoup(str(soup2.find(ttext, class_=ctext)).replace('<br/>', ' ')).get_text())\n",
    "                \n",
    "                #add number of contributions\n",
    "                c = soup2.findAll(tcontributions, class_=ccontributions)\n",
    "                assert len(c)==1 or len(c)==2, str(c)\n",
    "                if len(c) == 1:\n",
    "                    numcontributions.append(c[0].get_text())\n",
    "                    numupvotes.append(\"N/A\")\n",
    "                elif len(c) == 2:\n",
    "                    numcontributions.append(c[0].get_text())\n",
    "                    numupvotes.append(c[1].get_text())\n",
    "\n",
    "            \n",
    "                \n",
    "                #add the time of stay ttimestay  \n",
    "                t = soup2.find(ttimestay, class_=ctimestay)\n",
    "                timestays.append(t.get_text() if t is not None else \"N/A\")\n",
    "                \n",
    "                #add the location of the reviewer \n",
    "                l = soup2.find(treviewerloc, class_=creviewerloc)\n",
    "                reviewerlocs.append(l.get_text() if l is not None else \"N/A\")\n",
    "                \n",
    "                # add the reply from the hotel\n",
    "                r = soup2.find(treply, class_=creply)\n",
    "#                 replys.append(r.get_text() if r is not None else \"N/A\")\n",
    "                if r is not None:\n",
    "                    replys.append(BeautifulSoup(str(r).replace('<br/>', ' ')).get_text())\n",
    "                else:\n",
    "                    replys.append('N/A')\n",
    "                    \n",
    "#                     print(BeautifulSoup(str(r).replace('<br/>', ' ')).get_text())\n",
    "                    \n",
    "                # append trip type\n",
    "                tp = soup2.find(ttptype, class_=ctptype)\n",
    "                tptypes.append(tp.get_text()[11:] if tp is not None else \"N/A\")\n",
    "\n",
    "    \n",
    "            if not scroll:\n",
    "                break\n",
    "                \n",
    "            # go to the next page by clicking on \"next\"\n",
    "            next_buttons = driver.find_elements(By.XPATH, \"//*[@class='{next_page}']\".format(next_page = \"ui_button nav next primary \"))\n",
    "            if (len(next_buttons)) == 1:\n",
    "#                 print(\"Going to the next page...\")\n",
    "#                 next_buttons[0].click()\n",
    "                driver.execute_script(\"arguments[0].click();\", next_buttons[0])\n",
    "\n",
    "\n",
    "                # wait until page is completely loaded\n",
    "                time.sleep(random.uniform(sleepTime,sleepTime + 0.25))\n",
    "                while str(driver.execute_script(\"return document.readyState\")) != \"complete\" or not (driver.find_elements(By.XPATH, \"//*[@class='{expand}']\".format(expand = cexpand))[0].is_displayed()):\n",
    "                    time.sleep(random.uniform(0,0.25))\n",
    "                    \n",
    "                # expand reviews \n",
    "                expands = driver.find_elements(By.XPATH, \"//*[@class='{expand}']\".format(expand = cexpand))\n",
    "                assert expands[0].is_displayed(), \"expand url not properly loaded\"\n",
    "#                 expands[0].click()\n",
    "                driver.execute_script(\"arguments[0].click();\", expands[0])\n",
    "            else:\n",
    "#                 print(\"Reached the last page\")\n",
    "                break \n",
    "\n",
    "#         ids = np.arange(len(texts))\n",
    "        htnames = np.array([htname] * len(services))\n",
    "        \n",
    "        \n",
    "        assert len(tptypes) == len(replys) == len(reviewerlocs) == len(numupvotes) == len(timestays) == len(numcontributions) == len(htnames) == len(times) == len(names) == len(ratings) == len(titles) == len(values) == len(rooms) == len(locations) == len(cleanliness) == len(services) == len(slpqualitys) == len(texts), str(len(tptypes)) + str(len(replys)) + str(len(reviewerlocs)) + str(len(numupvotes)) + str(len(timestays)) + str(len(numcontributions))\n",
    "\n",
    "#         ids = np.array(ids)\n",
    "        texts = np.array(texts).astype('str')\n",
    "        values = np.array(values)\n",
    "        rooms = np.array(rooms)\n",
    "        locations = np.array(locations)\n",
    "        cleanliness = np.array(cleanliness)\n",
    "        services = np.array(services)\n",
    "        slpqualitys = np.array(slpqualitys)\n",
    "        times = np.array(times)\n",
    "        names = np.array(names)\n",
    "        ratings = np.array(ratings)\n",
    "        titles = np.array(titles)\n",
    "        numcontributions = np.array(numcontributions)\n",
    "        numupvotes = np.array(numupvotes)\n",
    "        timestays = np.array(timestays)\n",
    "        reviewerlocs = np.array(reviewerlocs)\n",
    "        replys = np.array(replys)\n",
    "        tptypes = np.array(tptypes).astype('str')\n",
    "        \n",
    "\n",
    "        allInfo = np.vstack([htnames,texts,values,rooms,locations,cleanliness,services,slpqualitys,times,names,ratings,titles,numcontributions, numupvotes, timestays,reviewerlocs, replys, tptypes])\n",
    "        print(\"shape of final array: \" + str(allInfo.shape))\n",
    "        \n",
    "        return allInfo\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def scrape(self, \n",
    "               url, \n",
    "               json_dir,\n",
    "               npy_dir,\n",
    "               final_name=\"final\",\n",
    "               scroll=False):\n",
    "        \"\"\"\n",
    "        Scrapes the results from a url or an array of urls.\n",
    "        \n",
    "        :param url: the url to the google review site. It must be either a String or a list/array of Strings.  \n",
    "        :param npy_dir: a numpy array containing the scraped reviews for each hotel will be saved in this directory.\n",
    "        :param json_dir: the final report containing all the scraped results will be saved in this directory.\n",
    "        :param final_name: the name the final file will be saved as.\n",
    "        # since Google only releases ten reviews each time it reloads, scrolling may take a long time for hotels with a large number of comments\n",
    "        :param scroll: whether or not to continue scrolling down until all the results are shown in the window and could be scraped, for trip advisor, whether or not to click \"next page\" until the last page. \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        assert (json_dir is not None) and (npy_dir is not None), \"both json_dir and npy_dir must be provided\" \n",
    "#         driver = webdriver.Chrome(self.path_to_driver)\n",
    "\n",
    "        driver = webdriver.Chrome(service=Service(self.path_to_driver), options=webdriver.ChromeOptions())\n",
    "        \n",
    "        if (self.site == \"Google_review\"):\n",
    "            if isinstance(url, str):\n",
    "                res = self.scrapeGoogle(url, driver, scroll=scroll)\n",
    "                \n",
    "                numReviews = res.shape[1]\n",
    "\n",
    "                #add timestamp\n",
    "                timestamp = np.array([str(datetime.datetime.now())]*numReviews)\n",
    "                finalArr = np.vstack([res, timestamp])\n",
    "\n",
    "                #save numpy array\n",
    "                np.savez_compressed(os.path.join(npy_dir, 'temp.npz').replace(\"\\\\\",\"/\"), a=finalArr)\n",
    "\n",
    "                # generate final report\n",
    "                headers = np.array(['ID', 'hotel', 'name', 'time', 'rating', 'site', 'trip type', 'room rating', 'location rating', 'service rating', 'text', 'hotel reply', 'reply date', 'timestamp']) \n",
    "                hotel = np.load(os.path.join(npy_dir, 'temp.npz').replace(\"\\\\\",\"/\"))[\"a\"].T\n",
    "                idx = np.array([np.arange(hotel.shape[0])]).T\n",
    "                hotel = np.hstack([idx, hotel])\n",
    "\n",
    "                # save as string and add indexes\n",
    "                df = pd.DataFrame(hotel, columns = headers).astype(str)\n",
    "                duplicate = 1\n",
    "                final_name += '0'\n",
    "                while True: \n",
    "                    if os.path.isfile(os.path.join(json_dir, final_name+'.json').replace(\"\\\\\",\"/\")):\n",
    "                        final_name = final_name[:-1] + str(duplicate)\n",
    "                        duplicate += 1 \n",
    "                        continue\n",
    "                    break\n",
    "\n",
    "                #df.to_csv(os.path.join(csv_dir, final_name + '.csv'), encoding='utf-8-sig', index=True)\n",
    "                df.to_json(os.path.join(json_dir, final_name + '.json').replace(\"\\\\\",\"/\"))\n",
    "                print(\"Scraped \" + str(numReviews) + \" reviews\")\n",
    "                \n",
    "            elif isinstance(url, (list, tuple, np.ndarray)):\n",
    "                headers = np.array(['ID','hotel', 'name', 'time', 'rating', 'site', 'trip type', 'room rating', 'location rating', 'service rating', 'text', 'hotel reply', 'reply date', 'timestamp']) \n",
    "                all_hotels = np.zeros(len(headers)-1) #-1:ID\n",
    "                np.savez_compressed(os.path.join(npy_dir, 'temp.npz').replace(\"\\\\\",\"/\"), a=all_hotels)\n",
    "                first = True\n",
    "                for index in range(len(url)):\n",
    "                    print('Scraping Target: ' + url[index])\n",
    "                    res = self.scrapeGoogle(url[index], driver, scroll=scroll)\n",
    "                    numReviews = res.shape[1]\n",
    "                    print(\"Scraped \" + str(numReviews) + \" reviews\")\n",
    "\n",
    "                    #add timestamp\n",
    "                    timestamp = np.array([str(datetime.datetime.now())]*numReviews)\n",
    "                    finalArr = np.vstack([res, timestamp])\n",
    "                    \n",
    "                    #load temp array\n",
    "                    hotels = np.load(os.path.join(npy_dir, 'temp.npz').replace(\"\\\\\",\"/\"))[\"a\"]\n",
    "                    if first:\n",
    "                        all_hotels = np.vstack([hotels, finalArr.T]).T\n",
    "                        first = False\n",
    "                    else:\n",
    "                         all_hotels = np.hstack([hotels, finalArr])\n",
    "                    \n",
    "                    #save temp array\n",
    "                    np.savez_compressed(os.path.join(npy_dir, 'temp.npz').replace(\"\\\\\",\"/\"), a=all_hotels)\n",
    "                   \n",
    "                    \n",
    "                # generate final report \n",
    "                all_hotels = np.load(os.path.join(npy_dir, 'temp.npz').replace(\"\\\\\",\"/\"))[\"a\"].T[1:, :]\n",
    "                idx = np.array([np.arange(all_hotels.shape[0])]).T\n",
    "                all_hotels = np.hstack([idx, all_hotels])\n",
    "                df = pd.DataFrame(all_hotels, columns = headers).astype(str)\n",
    "                duplicate = 1\n",
    "                final_name += '0'\n",
    "                while True: \n",
    "                    if os.path.isfile(os.path.join(json_dir, final_name+'.json').replace(\"\\\\\",\"/\")):\n",
    "                        final_name = final_name[:-1] + str(duplicate)\n",
    "                        duplicate += 1 \n",
    "                        continue\n",
    "                    break\n",
    "\n",
    "                #df.to_csv(os.path.join(csv_dir, final_name + '.csv').replace(\"\\\\\",\"/\"), encoding='utf-8-sig', index=True)\n",
    "                df.to_json(os.path.join(json_dir, final_name + '.json').replace(\"\\\\\",\"/\"))\n",
    "                \n",
    "                \n",
    "        elif (self.site == \"tpadvisor\"):\n",
    "            if isinstance(url, str):\n",
    "                res = self.scrapeAdvisor(url, driver, scroll=scroll)\n",
    "                numReviews = res.shape[1]\n",
    "                \n",
    "                #add timestamp\n",
    "                timestamp = np.array([str(datetime.datetime.now())]*numReviews)      \n",
    "                finalArr = np.vstack([res, timestamp])\n",
    "\n",
    "                #save numpy array\n",
    "                np.savez_compressed(os.path.join(npy_dir, 'temp.npz').replace(\"\\\\\",\"/\"), a=finalArr)\n",
    "\n",
    "                # generate final report   added: numcontributions, numupvotes, timestays,reviewerlocs, replys, tptypes\n",
    "                headers = np.array(['ID','hotel name','review text','value rating','room rating','location rating','cleanliness rating','service rating','sleep quality','date','rater name','overall rating','rating title', 'number of contributions','number of upvotes', 'time of stay','reviewer location', 'hotel reply', 'trip type', 'timestamp']) \n",
    "                hotel = np.load(os.path.join(npy_dir, 'temp.npz').replace(\"\\\\\",\"/\"))[\"a\"].T\n",
    "                idx = np.array([np.arange(hotel.shape[0])]).T\n",
    "                hotel = np.hstack([idx, hotel])\n",
    "\n",
    "                # save as string and add indexes\n",
    "                df = pd.DataFrame(hotel, columns = headers).astype(str)\n",
    "                duplicate = 1\n",
    "                final_name += '0'\n",
    "                while True: \n",
    "                    if os.path.isfile(os.path.join(json_dir, final_name+'.json').replace(\"\\\\\",\"/\")):\n",
    "                        final_name = final_name[:-1] + str(duplicate)\n",
    "                        duplicate += 1 \n",
    "                        continue\n",
    "                    break\n",
    "\n",
    "                #df.to_csv(os.path.join(csv_dir, final_name+'.csv'), encoding='utf-8-sig', index=True)\n",
    "                df.to_json(os.path.join(json_dir, final_name + '.json').replace(\"\\\\\",\"/\"))\n",
    "                print(\"Scraped \" + str(numReviews) + \" reviews\")\n",
    "\n",
    "            elif isinstance(url, (list, tuple, np.ndarray)):\n",
    "                headers = np.array(['ID','hotel name','review text','value rating','room rating','location rating','cleanliness rating','service rating','sleep quality','date','rater name','overall rating','rating title', 'number of contributions','number of upvotes', 'time of stay','reviewer location', 'hotel reply', 'trip type', 'timestamp']) \n",
    "                all_hotels = np.arange(len(headers)-1) # -1:no ID here \n",
    "                np.savez_compressed(os.path.join(npy_dir, 'temp.npz').replace(\"\\\\\",\"/\"), a=all_hotels)\n",
    "                first = True\n",
    "                for index in range(len(url)):\n",
    "                    print('Scraping Target: ' + url[index])\n",
    "                    res = self.scrapeAdvisor(url[index], driver, scroll=scroll)\n",
    "                    numReviews = res.shape[1]\n",
    "                    print(\"Scraped \" + str(numReviews) + \" reviews\")\n",
    "\n",
    "                    #add timestamp\n",
    "                    timestamp = np.array([str(datetime.datetime.now())]*numReviews)\n",
    "                    finalArr = np.vstack([res, timestamp])\n",
    "                    \n",
    "                    #load temp array\n",
    "                    hotels = np.load(os.path.join(npy_dir, 'temp.npz').replace(\"\\\\\",\"/\"))[\"a\"]\n",
    "\n",
    "                    if first:\n",
    "                        all_hotels = np.vstack([hotels, finalArr.T]).T\n",
    "                        first = False\n",
    "                    else:\n",
    "                         all_hotels = np.hstack([hotels, finalArr])\n",
    "                    \n",
    "                    #save temp array\n",
    "                    np.savez_compressed(os.path.join(npy_dir, 'temp.npz').replace(\"\\\\\",\"/\"), a=all_hotels)\n",
    "                    \n",
    "                # generate final report \n",
    "                all_hotels = np.load(os.path.join(npy_dir, 'temp.npz').replace(\"\\\\\",\"/\"))[\"a\"].T[1:, :]\n",
    "                idx = np.array([np.arange(all_hotels.shape[0])]).T\n",
    "                all_hotels = np.hstack([idx, all_hotels])\n",
    "                df = pd.DataFrame(all_hotels, columns = headers).astype(str)\n",
    "                \n",
    "                duplicate = 1\n",
    "                final_name += '0'\n",
    "                while True: \n",
    "                    if os.path.isfile(os.path.join(json_dir, final_name+'.json').replace(\"\\\\\",\"/\")):\n",
    "                        final_name = final_name[:-1] + str(duplicate)\n",
    "                        duplicate += 1 \n",
    "                        continue\n",
    "                    break\n",
    "\n",
    "                #df.to_csv(os.path.join(csv_dir, final_name+'.csv').replace(\"\\\\\",\"/\"), encoding='utf-8-sig', index=True)\n",
    "                df.to_json(os.path.join(json_dir, final_name + '.json').replace(\"\\\\\",\"/\"))\n",
    "        else:\n",
    "            print(\"We are sorry, this website is currently not supported\")\n",
    "            return\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y36GaF6NRyDU"
   },
   "outputs": [],
   "source": [
    "def scroll_down(driver, scrollDistanceToBottom=700, sleepTime=2.25):\n",
    "    \"\"\"\n",
    "    A method for scrolling the page.\n",
    "    :param driver: the web driver object.\n",
    "\n",
    "    # Tune these parameters according to your browser and screen\n",
    "    :param scrollDistanceToBottom: Google does not load more results if you directly scroll to the very bottom. This parameter serves as the margin between the location we scroll to and the bottom of the window.\n",
    "    :param sleepTime: the scrolling stops when no more information is loaded after waiting for a certain time past the previous scroll. sleepTime will be the time we wait for the webpage to finish reloading. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Get scroll height.\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # Scroll down to the bottom.\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight-\"+ str(scrollDistanceToBottom) + \");\")\n",
    "\n",
    "        # Wait to load the page.\n",
    "        time.sleep(random.uniform(sleepTime,sleepTime+0.25))\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height.\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "\n",
    "        last_height = new_height\n",
    "        \n",
    "# this scrolling function is inspired by Ratmir Asanov's answer at https://stackoverflow.com/questions/48850974/selenium-scroll-to-end-of-page-in-dynamically-loading-webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fK-rknYmRyDV"
   },
   "outputs": [],
   "source": [
    "def loadCheckpoint(checkpoint_path, json_dir, final_name, site, multiple_urls=True):\n",
    "    \"\"\"converts a checkpoint npz file into a json file\"\"\"\n",
    "    temp = np.load(checkpoint_path)[\"a\"]\n",
    "    headers=[]\n",
    "    if site==\"tpadvisor\":\n",
    "        headers = np.array(['ID','hotel name','review text','value rating','room rating','location rating','cleanliness rating','service rating','sleep quality','date','rater name','overall rating','rating title', 'number of contributions','number of upvotes', 'time of stay','reviewer location', 'hotel reply', 'trip type', 'timestamp']) \n",
    "    elif site==\"Google_review\":\n",
    "        headers = np.array(['ID', 'hotel', 'name', 'time', 'rating', 'site', 'trip type', 'room rating', 'location rating', 'service rating', 'text', 'hotel reply', 'reply date', 'timestamp'])\n",
    "    else: \n",
    "        print(\"site not supported\")\n",
    "        return\n",
    "    \n",
    "    print(\"Loading checkpoint...\")\n",
    "    # generate final report \n",
    "    all_hotels = np.load(checkpoint_path)[\"a\"].T[multiple_urls:, :]\n",
    "    idx = np.array([np.arange(all_hotels.shape[0])]).T\n",
    "    all_hotels = np.hstack([idx, all_hotels])\n",
    "    df = pd.DataFrame(all_hotels, columns = headers).astype(str)\n",
    "\n",
    "    duplicate = 1\n",
    "    final_name += '0'\n",
    "    while True: \n",
    "        if os.path.isfile(os.path.join(json_dir, final_name+'.json').replace(\"\\\\\",\"/\")):\n",
    "            final_name = final_name[:-1] + str(duplicate)\n",
    "            duplicate += 1 \n",
    "            continue\n",
    "        break\n",
    "\n",
    "    #df.to_csv(os.path.join(csv_dir, final_name+'.csv').replace(\"\\\\\",\"/\"), encoding='utf-8-sig', index=True)\n",
    "    df.to_json(os.path.join(json_dir, final_name + '.json').replace(\"\\\\\",\"/\"))\n",
    "    print(\"Success!\") \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rn4rvtGURyDW"
   },
   "source": [
    "## Let's Scrape some Reviews with our ReviewScraper from Google\n",
    "\n",
    "We need to provide some information to the ReviewScraper:\n",
    "\n",
    "* **path_to_driver**: The path to your chromedriver.exe file (String).  \n",
    "\n",
    "\n",
    "* **json_dir**: where to save the file that contains all the scraped content (String, path to a folder).    \n",
    "\n",
    "\n",
    "* **npy_dir**: where the temp.npz checkpoint will be saved. This file contains all the previous scraped records in case of errors or internet problems. (String, path to a folder). Note that this file will be overwritten each time the scrape() function is run (in the same npy_dir). If you want to convert the checkpoint into a json file, please use loadCheckpoint() punction (more details later).  \n",
    "\n",
    "\n",
    "* **site**: The review site you want to scrape from. (String, It must be either \"Google_review\", or \"tpadvisor\").  \n",
    "\n",
    "\n",
    "* **final_name**: The file name to which the scraped data is to be saved as (String).  \n",
    "\n",
    "\n",
    "* **scroll**: whether or not to get **all** the results available for each hotel (Boolean). If scroll=True, the scraper will repeatedly scroll down or press \"next page\" until every review is available for scraping. This may take an very long time since we need to wait for the page to load every time new reviews are loaded. If scroll=False, the scraper will not attempt to load any more reviews other than the ones present in the first page of the website.\n",
    "\n",
    "\n",
    "* **url**: The url to the hotel's review webpage (String, it could be either a single url to a single hotel property, or a list of urls. Make sure that the urls are from the review website (Google vs. TripAdvisor) that you specified above in ***site***.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2IPnl51RyDW"
   },
   "outputs": [],
   "source": [
    "# 1 Initialize File Locations\n",
    "\n",
    "# 1a Please specify the path to your chromedriver file here (include filename and extension of chromedriver!)\n",
    "chromedrive_path = r'C:\\Users\\zs\\Desktop\\forIntern\\chromedriver.exe' # replace with your local folder\n",
    "\n",
    "# 1b Please enter the paths where you want the scraped data to be stored (do not include filenames here!)\n",
    "json_dir=r'C:\\Users/zs/Desktop/forIntern/junk' # replace with your local folder\n",
    "npy_dir=r'C:/Users/zs/Desktop/forIntern/junk' # replace with your local folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4riO4CyxRyDW"
   },
   "source": [
    "### <font color='red'>Important:</font>\n",
    "    \n",
    "1. When you run the cell below, you will see a Chrome browser window pop up. Look at it and make sure that the page is scrolling down repeatedly for more information.   \n",
    "    \n",
    "    \n",
    "2. Except for viweing, ***do not interact with the Chrome browser window in any*** other way. Clicking, scrolling or resizing may cause the scraper to not work properly.  \n",
    "    \n",
    "    \n",
    "3. If you are running code that would take a long time, ***please make sure that your computer does not go to sleep*** or that your screen turns off in the middle. This will also cause problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7OOqsegdRyDX"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 2a Define a single url\n",
    "url='https://www.google.com/travel/hotels/graduate%20chapel%20hill/entity/CgsInqmaiMnBi4yIARAB/reviews?g2lb=4722772%2C4597339%2C4647135%2C4317915%2C2503781%2C2503946%2C4605861%2C4641139%2C4258168%2C2503771%2C2502548%2C4649665%2C4270442%2C4401769%2C4640247%2C4306835%2C4596364%2C4270859%2C4284970%2C4291517&hl=en-US&gl=us&ssta=1&q=graduate%20chapel%20hill&rp=EJ6pmojJwYuMiAEQnqmaiMnBi4yIATgCQABIAcABApoCAggA&ictx=1&sa=X&ved=0CAAQ5JsGahcKEwigwpLoi5D2AhUAAAAAHQAAAAAQAg&utm_campaign=sharing&utm_medium=link&utm_source=htls&ts=CAESABpJCisSJzIlMHg4OWFjYzJlMTIxYzU5OTVmOjB4ODgxODJlMGM5MTA2OTQ5ZRoAEhoSFAoHCOYPEAIYGxIHCOYPEAIYHBgBMgIQACoJCgU6A1VTRBoA'\n",
    "\n",
    "# 2b Define filename for results (no file extension!)\n",
    "filename='singlegoogleurl'\n",
    "\n",
    "# 2c Instantiate ReviewScraper\n",
    "scraper = ReviewScraper(chromedrive_path, site=\"Google_review\")\n",
    "\n",
    "# 2d Scrape all possible results for this hotel (this will take some time)\n",
    "scraper.scrape(url=url, \n",
    "               json_dir=json_dir, \n",
    "               npy_dir=npy_dir,\n",
    "               final_name=filename,\n",
    "               scroll=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_biG5pfzRyDX"
   },
   "source": [
    "#### **Once the scraping process finished, you should have collected more than 300 reviews.**\n",
    "\n",
    "The reviews are saved to a JSON file in the folder that your specified (json_dir folder). \n",
    "\n",
    "***If you scraped less than 300 reviews for Graduate Hotel Chapel Hill, look into these possible causes:***\n",
    "1. the browser loads the new reviews too slowly, so the scraper thinks that there are no more comments and stops scrolling down. A way to figure out whether this is the problem is to rerun the above cell. Check whether you get a different number of reviews each time. You could fix this problem by modifying the \"slptime\" variable in the function \"scrapeGoogle\" in the ReviewScraper class. (ReviewScraper is able to auto-adjust the sleeping time if the loading time isn't __too__ slow. Whenever the sleeping time is adjusted you will get the \"your sleeptime may be too slow\" message.)\n",
    "2. The three problems specified in <font color='red'>\"Important:\"</font> in the previous instruction cell.\n",
    "3. Random website or internet outages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpWoEPNtRyDX"
   },
   "source": [
    "#### It is good practice to verify your scraping results before you scrape a large number of reviews.\n",
    "\n",
    "- Inspect the scraped data in the json file and compare reviews to those you find when you go to the website you scraped off in your browser. \n",
    "\n",
    "#### If there are inconsistencies, the following might have occured:\n",
    "1. Update by Google or tripadvisor. In the ReviewScraper class, the scrapeAdvisor() and scrapeGoogle() function contains various variables/parameters that contain the information about the HTML tags containing the information we need to scrape. These variables and parameters need to be updated (see Appendix).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gptx76sqG81R"
   },
   "outputs": [],
   "source": [
    "# 1 Import scraped data (I am constructing the path and filename here)\n",
    "test= pd.read_json(json_dir + '/' + filename +'0.json')\n",
    "\n",
    "    # Alternatively (if file in current path)\n",
    "    # test= pd.read_json('yourfilename.json')\n",
    "\n",
    "# 2 Check random sample of 10 reviews\n",
    "display(test.sample(10))\n",
    "\n",
    "# 3 Print first couple of review texts\n",
    "[print(f'{x} \\n') for x in test.text.values[0:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4xzWJCRRyDX"
   },
   "source": [
    "## Scrape a list of URLs from Google\n",
    "\n",
    "The urls below are review pages on google travel for several graduate hotels. You could replace them with other review urls:\n",
    "1. enter the **exact full** name of the hotel in a google searchbar and search.\n",
    "2. click on \"**google reviews**\" (see below)\n",
    "![Google Review](https://mapxp.app/MBA742/google-review.png)\n",
    "3. copy the url to the current page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CseR-ZARyDY"
   },
   "outputs": [],
   "source": [
    "# 1 Specify list of URLS for several hotel reviews on Google\n",
    "urls = ['https://www.google.com/travel/hotels/entity/CgoIyZLQmLyR4uJREAE/reviews?g2lb=2503771%2C4644488%2C4596364%2C4597339%2C4419364%2C4317915%2C4270442%2C4371335%2C4645479%2C4306835%2C4641139%2C4605861%2C4401769%2C4624411%2C4258168%2C4671810%2C2503781%2C2502548%2C4640247%2C4672717%2C4284970%2C4270859%2C4291517&hl=en-US&gl=us&ssta=1&rp=EMmS0Ji8keLiURDJktCYvJHi4lE4AkAASAHAAQI&ictx=1&sa=X&sqi=2&ved=0CAAQ5JsGahcKEwi4lq39xrf0AhUAAAAAHQAAAAAQAg&utm_campaign=sharing&utm_medium=link&utm_source=htls&ts=CAESABpJCisSJzIlMHg4ODNjYWUzZmYxMmIzNjYxOjB4NTFjNTg4OGJjMzE0MDk0ORoAEhoSFAoHCOUPEAsYHRIHCOUPEAsYHhgBMgIQACoJCgU6A1VTRBoA',\n",
    "       'https://www.google.com/travel/hotels/entity/CgsIi5HS6P-dzdyXARAB/reviews?g2lb=4672717%2C4640247%2C2503781%2C2502548%2C4671810%2C4258168%2C4401769%2C4624411%2C4605861%2C4306835%2C4641139%2C4371335%2C4645479%2C4270442%2C4317915%2C4597339%2C4419364%2C4644488%2C4596364%2C2503771%2C4291517%2C4270859%2C4284970&hl=en-US&gl=us&ssta=1&rp=EIuR0uj_nc3clwEQi5HS6P-dzdyXATgCQABIAcABAg&ictx=1&sa=X&ved=0CAAQ5JsGahcKEwio8_Cnx7f0AhUAAAAAHQAAAAAQAg&utm_campaign=sharing&utm_medium=link&utm_source=htls&ts=CAESABpJCisSJzIlMHg4OWI3ZjdlMTUzNmY1MzI1OjB4OTdiOTM0ZWZmZDE0ODg4YhoAEhoSFAoHCOUPEAwYChIHCOUPEAwYDBgCMgIQACoJCgU6A1VTRBoA',\n",
    "       'https://www.google.com/travel/hotels/entity/CgoIv5fgkq67qdNmEAE/reviews?g2lb=2503781%2C2502548%2C4672717%2C4640247%2C4258168%2C4401769%2C4624411%2C4671810%2C4371335%2C4645479%2C4270442%2C4317915%2C4605861%2C4306835%2C4641139%2C4644488%2C4596364%2C2503771%2C4597339%2C4419364%2C4270859%2C4291517%2C4284970&hl=en-US&gl=us&ssta=1&rp=EL-X4JKuu6nTZhC_l-CSrrup02Y4AkAASAHAAQI&ictx=1&sa=X&ved=0CAAQ5JsGahcKEwjYmuzox7f0AhUAAAAAHQAAAAAQAg&utm_campaign=sharing&utm_medium=link&utm_source=htls&ts=CAESCgoCCAMKAggDEAEaSQorEicyJTB4ODhmNjZjZDllNWE3MTgzOToweDY2YTZhNWRhZTI1ODBiYmYaABIaEhQKBwjlDxAMGA0SBwjlDxAMGA4YATICEAAqCQoFOgNVU0QaAA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDbrLlPpRyDY"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 2 Set filename for output file\n",
    "filename = 'multiplegoogleurls'\n",
    "\n",
    "# 3 Instantiate ReviewScraper \n",
    "scraper = ReviewScraper(chromedrive_path, \"Google_review\")\n",
    "\n",
    "# 4 Start scraping\n",
    "print(\"Depending on the total number of URLs and Reviews, this may take a long time.\")\n",
    "print(\"Do not allow your computer or screen to go to sleep during the scraping process.\")\n",
    "scraper.scrape(url=urls, \n",
    "               json_dir=json_dir, \n",
    "               npy_dir=npy_dir,\n",
    "               final_name=filename,\n",
    "               scroll=False) # \"scroll\" is being set to false in order to save time! If you want to collect all reviews, se this to True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIAf63GlRyDY"
   },
   "source": [
    "- After the execution, a json file containing all the results, as well as a temp.npz file will be saved in respective directories. The json should contain about 30 entries.  \n",
    "  \n",
    "- If we set scroll=True in the cell above, we will be scraping every google review for every hotel in the URL list. The execution will take a long time and you should end up with all reviews available online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9V4tuGRRyDY"
   },
   "source": [
    "## Scrape Reviews for a single Hotel from TripAdvisor \n",
    "\n",
    "**Note** Scraping TripAdvisor is a lot slower!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boDMXm2tRyDY",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 1 Set URL to scrape\n",
    "url = \"https://www.tripadvisor.com/Hotel_Review-g38020-d91957-Reviews-Graduate_Iowa_City-Iowa_City_Iowa.html\"\n",
    "\n",
    "# 2 Set filename for output\n",
    "filename='singletripadvisorurl'\n",
    "\n",
    "# 3 Instatiate ReviewScraper\n",
    "scraper = ReviewScraper(chromedrive_path, site=\"tpadvisor\")\n",
    "\n",
    "# 4 Scrape reviews (this will take some time)\n",
    "scraper.scrape(url=url, \n",
    "               json_dir=json_dir, \n",
    "               npy_dir=npy_dir,\n",
    "               final_name=filename,\n",
    "               scroll=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTNizIEjRyDZ"
   },
   "source": [
    "After executing, you should get 220+ results. The sleeptime in this case (default=2.5s) is the \"sleepTime\" variable in the \"scrapeAdvisor\" function of the ReviewScraper class. Please adjust if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RK5YXOOZG81T"
   },
   "outputs": [],
   "source": [
    "# 1 Import scraped data (I am constructing the path and filename here)\n",
    "test= pd.read_json(json_dir + '/' + filename +'0.json')\n",
    "\n",
    "    # Alternatively (if file in current path)\n",
    "    # test= pd.read_json('yourfilename.json')\n",
    "\n",
    "# 2 Check random sample of 10 reviews\n",
    "display(test.sample(10))\n",
    "\n",
    "# 3 Print first couple of review texts\n",
    "[print(f'{x} \\n') for x in test['review text'].values[0:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehpYKMZLRyDZ"
   },
   "source": [
    "## Scrape Reviews from a List of Hotels from Tripadvisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1ZVzfnMRyDZ"
   },
   "source": [
    "#### Just as with Google reviews, you could replace these urls with your own:\n",
    "\n",
    "1. Go to https://www.tripadvisor.com/\n",
    "2. In the searchbar, enter the **exact full** name of the hotel.\n",
    "3. Click on the search result for that particular hotel (see below)\n",
    "![TripAdvisor Review](https://mapxp.app/MBA742/tripadvisor-review.png)\n",
    "4. Copy the current url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyMVkaN-RyDZ"
   },
   "outputs": [],
   "source": [
    "# 1 Define list of URLs for multiple hotels on trip advisor \n",
    "urls2 = ['https://www.tripadvisor.com/Hotel_Review-g29556-d89935-Reviews-Graduate_Ann_Arbor-Ann_Arbor_Michigan.html',\n",
    "        'https://www.tripadvisor.com/Hotel_Review-g29494-d89319-Reviews-Graduate_Annapolis-Annapolis_Maryland.html',\n",
    "        'https://www.tripadvisor.com/Hotel_Review-g29209-d242422-Reviews-Graduate_Athens-Athens_Georgia.html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ecbk8i_RyDZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 2 Set filename for output\n",
    "filename = 'multipletripadvisorurls'\n",
    "\n",
    "# 3 Instatiate ReviewScraper for TripAdvisor\n",
    "scraper = ReviewScraper(chromedrive_path, site=\"tpadvisor\")\n",
    "\n",
    "# 4 Scrape Reviews (here, we do not scroll to save time)\n",
    "scraper.scrape(url=urls2, \n",
    "               json_dir=json_dir, \n",
    "               npy_dir=npy_dir,\n",
    "               final_name=filename,\n",
    "               scroll=False) # (set to false to save time - set to True to get all reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxyLDGy9RyDZ"
   },
   "source": [
    "- After the execution, a json file containing all scraped reviews as well as a temp.npz file will be saved in respective directories.  \n",
    "  \n",
    "- The final json file should contain about 15 entries.  \n",
    "  \n",
    "- If we set scroll=True in the cell above, we will be scraping every tripadvisor review for each hotel in the URL list. The execution will take a long time and you should end up with many more reviews! \n",
    "\n",
    "- Don't forget to keep the chrome browser window that pops-up open, do not manipualte it, and do not allow your computer to go to sleep!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1L1tdRsURyDa"
   },
   "source": [
    "## Continue Scraping after an early Termination\n",
    "\n",
    "- Sometimes, the ReviewScraper might terminate early (due to internet problems, errors, etc.) In this case, you don't have all the data collected and no json file was written.\n",
    "\n",
    "- Use the loadCheckpoint() function to help you convert the checkpoint temp.npz file into a json report so that you don't need to start over from the very beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwOM_EZaRyDa"
   },
   "outputs": [],
   "source": [
    "loadCheckpoint(checkpoint_path=npy_dir+\"/temp.npz\", # the path to the checkpoint file\n",
    "               json_dir=json_dir, # the folder to save the generated report \n",
    "               final_name=\"loaded_checkpoint\", # the name of the generated report \n",
    "               site=\"Google_review\", # specify the review website \n",
    "               multiple_urls=True) # whether you are scraping multiple urls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cZ6wT4YwW4s"
   },
   "source": [
    "## Convert to CSV\n",
    "**IF** you, *for whatever reason (??? !!!)* need to convert the scraped data (which is stored in json format) into a CSV file, then you can use the code below. \n",
    "- The code imports the json file to a dataframe\n",
    "- Saves the dataframe to a CSV file  \n",
    "\n",
    "**IF** you open the CSV, for example with Excel, and save it again, then your data will likely be corrupted (i.e., data becomes all jumbled up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmvXvNoEu05j"
   },
   "outputs": [],
   "source": [
    "# convert json to csv (make sure the filename actually exists in your json path)\n",
    "filename = r'multipletripadvisorurls0'\n",
    "j = pd.read_json(json_dir + '/' + filename + '.json')\n",
    "j.to_csv(json_dir + '/' + filename + '.csv', encoding='utf-8-sig', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjrLPcgGRyDa"
   },
   "source": [
    "# Appendix: Fixing the scraper in the case of website updates\n",
    "\n",
    "At the beginning of the scrapeAdvisor() and scrapeGoogle() function, I have specified the identifiers for all the tags we need. Variable names starting with 't' contains the type of the tag and variables starting with 'c' contains the class of the tag. \n",
    "\n",
    "For example, the tag above contains the overall rating, so we have crating = \"MfbzKb\" and trating = 'div'. \n",
    "You may need to change these identifiers if Google updates the website."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
